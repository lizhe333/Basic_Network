{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5673f2",
   "metadata": {},
   "source": [
    "# 基础结构的ResNet block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa96046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入与输出是一个维度，恒等映射的情况下stride=1: torch.Size([64, 64, 224, 224])\n",
      "输入与输出维度不同，残差连接部分需要进行变化stride=2: torch.Size([64, 64, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "每个BasicBlock由两个卷积层组成，每个卷积层后面跟着一个批量归一化层和ReLU激活函数。\n",
    "维度保持不变，即输入和输出的通道数相同。\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride=1):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels,out_channels,3,stride,1)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "        self.conv2=nn.Conv2d(out_channels,out_channels,3,1,1)\n",
    "        self.bn2=nn.BatchNorm2d(out_channels)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        #shortcut部分\n",
    "        #默认是一个恒等映射\n",
    "        self.shortcut=nn.Sequential()\n",
    "        #如果输入与输出维度不一样，则需要调整\n",
    "        if stride!=1 or in_channels!=out_channels:\n",
    "            self.shortcut=nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,1,stride,0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.relu(self.bn1(self.conv1(x)))\n",
    "        out=self.bn2(self.conv2(out))\n",
    "        out+=self.shortcut(x)\n",
    "        out=self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "x=torch.randn(64,3,224,224)\n",
    "residual_block1=ResidualBlock(3,64)\n",
    "out1=residual_block1(x)\n",
    "residual_block2=ResidualBlock(3,64,2)\n",
    "out2=residual_block2(x)\n",
    "print(f\"输入与输出是一个维度，恒等映射的情况下stride=1: {out1.shape}\")\n",
    "print(f\"输入与输出维度不同，残差连接部分需要进行变化stride=2: {out2.shape}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa41b89",
   "metadata": {},
   "source": [
    "## 为了使网络加深，同时节约计算量，使用一个bottleneck结构\n",
    "\n",
    "bottleneck的核心是先降维，再提取特征，再升维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08bcb202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    '''\n",
    "    瓶颈结构的ResNet block\n",
    "    :param inplanes:输入通道数\n",
    "    :param planes:输出通道数\n",
    "    :param stride:步长\n",
    "    :param downsample:下采样层\n",
    "    downsample是为了处理当主分支的输出维度与残差分支的输出维度不同时，需要进行下采样的情况\n",
    "    通常是用于判断残差流是用identity block还是conv block\n",
    "    '''\n",
    "    expansion=4#扩展后的维度的倍数,实际的输出通道数是planes*expansion\n",
    "    def __init__(self,inplanes,planes,stride=1,downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(inplanes,planes,kernel_size=1,stride=1,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2=nn.Conv2d(planes,planes,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv3=nn.Conv2d(planes,planes*self.expansion,kernel_size=1,stride=1,bias=False)\n",
    "        self.bn3=nn.BatchNorm2d(planes*self.expansion)\n",
    "\n",
    "        self.relu=nn.ReLU(inplace=False)\n",
    "        self.downsample=downsample\n",
    "        self.stride=stride\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        在这里实现残差块的连接，需要根据downsample是否为None来判断是否需要进行下采样\n",
    "        如果是Identity Block，直接将输入x添加到输出上\n",
    "        如果是Conv Block，需要先进行下采样，然后再添加到输出上\n",
    "        identity是恒等映射，可以串联，增加网络深度\n",
    "        conv是改变维度，无法继续串联\n",
    "        '''\n",
    "        residual=x# 残差连接的输入\n",
    "        #print(f\"residual.shape={residual.shape}\")\n",
    "        #print(f\"x.shape={x.shape}\")\n",
    "\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out=self.relu(out)\n",
    "        #print(f\"conv1.shape={out.shape}\")\n",
    "        out=self.conv2(out)\n",
    "        out=self.bn2(out)\n",
    "        out=self.relu(out)\n",
    "        #print(f\"conv2.shape={out.shape}\")\n",
    "        out=self.conv3(out)\n",
    "        out=self.bn3(out)\n",
    "        #print(f\"conv3.shape={out.shape}\")\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            #需要对残差块进行下采样\n",
    "            residual=self.downsample(residual)\n",
    "            #print(f\"downsample.shape={residual.shape}\")\n",
    "        # 残差连接\n",
    "        out+=residual\n",
    "        out=self.relu(out)\n",
    "        print(f\"out.shape={out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed968d6",
   "metadata": {},
   "source": [
    "# 搭建完整的残差网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab90f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入尺寸: torch.Size([1, 3, 224, 224])\n",
      "初始化卷积层输出尺寸: torch.Size([1, 64, 112, 112])\n",
      "初始化池化层输出尺寸: torch.Size([1, 64, 56, 56])\n",
      "out.shape=torch.Size([1, 256, 56, 56])\n",
      "out.shape=torch.Size([1, 256, 56, 56])\n",
      "Layer1输出尺寸: torch.Size([1, 256, 56, 56])\n",
      "out.shape=torch.Size([1, 512, 28, 28])\n",
      "out.shape=torch.Size([1, 512, 28, 28])\n",
      "Layer2输出尺寸: torch.Size([1, 512, 28, 28])\n",
      "out.shape=torch.Size([1, 1024, 14, 14])\n",
      "out.shape=torch.Size([1, 1024, 14, 14])\n",
      "Layer3输出尺寸: torch.Size([1, 1024, 14, 14])\n",
      "out.shape=torch.Size([1, 2048, 7, 7])\n",
      "out.shape=torch.Size([1, 2048, 7, 7])\n",
      "Layer4输出尺寸: torch.Size([1, 2048, 7, 7])\n",
      "自适应平均池化层输出尺寸: torch.Size([1, 2048, 1, 1])\n",
      "展平层输出尺寸: torch.Size([1, 2048])\n",
      "全连接层输出尺寸: torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0722,  0.1190, -0.1511,  0.9905,  0.3131,  0.4621,  0.0034, -0.7411,\n",
       "          0.2812,  0.0759]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block,layers,num_classes=10):\n",
    "        super().__init__()\n",
    "        # -----------------------------------#\n",
    "        #   假设输入进来的图片是3，224，224\n",
    "        # -----------------------------------#\n",
    "        self.inplanes=64\n",
    "        '''\n",
    "        ResNet共同的初始化操作都是：\n",
    "        1.卷积层,使用大卷积核，减半尺寸\n",
    "        2.批量归一化层\n",
    "        3.激活函数层\n",
    "        4.池化层，最大池化，继续减半尺寸\n",
    "        目的是：迅速降低图像的分辨率，同时增加通道数\n",
    "        '''\n",
    "        self.conv1=nn.Conv2d(3,self.inplanes,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        #定义残差块，只在第一个块中使用下采样\n",
    "        self.layer1=self._make_layer(block,64,layers[0],stride=1)\n",
    "    \n",
    "        self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
    "        self.layer3=self._make_layer(block,256,layers[2],stride=2)\n",
    "        self.layer4=self._make_layer(block,512,layers[3],stride=2)\n",
    "\n",
    "        #自适应平均池化层，将特征图的尺寸调整为1x1\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
    "        #全连接层，将特征图的通道数映射到类别数\n",
    "        self.fc=nn.Linear(512*block.expansion,num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        '''\n",
    "        搭建一个残差块层\n",
    "        block:残差块的类型\n",
    "        planes*expansion:输出通道数\n",
    "        blocks:残差块的数量\n",
    "        stride:步长\n",
    "        return :构造好的残差块层\n",
    "        注意：在使用了 Bottleneck 的 ResNet中，\n",
    "        每一层（Layer 1 到 Layer 4）的第一个 Block 都是 Conv Block，\n",
    "        后面的才是 Identity Block。\n",
    "\n",
    "        '''\n",
    "        layers=[]\n",
    "        #定义下采样块来处理输入和输出通道数不同的情况\n",
    "        if stride!=1 or self.inplanes!=planes*block.expansion:\n",
    "            downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes,planes*block.expansion,1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(planes*block.expansion)\n",
    "            )\n",
    "        else:\n",
    "            downsample=None\n",
    "        #第一层是Conv block，其余的是Identity block\n",
    "        layers.append(block(self.inplanes,planes,stride,downsample))\n",
    "        #更新通道数\n",
    "        self.inplanes=planes*block.expansion\n",
    "        #添加剩余的残差块\n",
    "        for _ in range(1,blocks):\n",
    "            layers.append(block(self.inplanes,planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        print(f\"输入尺寸: {x.shape}\")\n",
    "        # 1. 卷积层, 使用大卷积核, 减半尺寸\n",
    "        x=self.conv1(x)\n",
    "        print(f\"初始化卷积层输出尺寸: {x.shape}\")\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        print(f\"初始化池化层输出尺寸: {x.shape}\")\n",
    "        x=self.layer1(x)\n",
    "        print(f\"Layer1输出尺寸: {x.shape}\")\n",
    "        x=self.layer2(x)\n",
    "        print(f\"Layer2输出尺寸: {x.shape}\") \n",
    "        x=self.layer3(x)\n",
    "        print(f\"Layer3输出尺寸: {x.shape}\")\n",
    "        x=self.layer4(x)    \n",
    "        print(f\"Layer4输出尺寸: {x.shape}\")\n",
    "        # 自适应平均池化层, 将特征图的尺寸调整为1x1\n",
    "        x=self.avgpool(x)\n",
    "        print(f\"自适应平均池化层输出尺寸: {x.shape}\")\n",
    "        # 展平层, 将特征图展平为一维向量\n",
    "        x=torch.flatten(x,1)#从第一个维度开始展开\n",
    "        print(f\"展平层输出尺寸: {x.shape}\")\n",
    "        # 全连接层, 将特征图的通道数映射到类别数\n",
    "        x=self.fc(x)\n",
    "        print(f\"全连接层输出尺寸: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "X=torch.randn(1,3,224,224)# 批量大小为 3，通道数为 3，图像尺寸为 224x224\n",
    "model=ResNet(Bottleneck,[2,2,2,2])\n",
    "model(X) # Resnet18 18是指有权重的层数 比如卷积层和线性层 1+16+1=18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
