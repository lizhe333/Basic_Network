{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ec14f4",
   "metadata": {},
   "source": [
    "## 实现Vision Transformer\n",
    "\n",
    "分为以下几个部分：\n",
    "1. PatchEmbed 类：图像嵌入层，将输入图像分割为多个patch并进行线性投影。\n",
    "\n",
    "2. Attention 类：自注意力机制层，用于计算每个patch之间的注意力权重。\n",
    "\n",
    "3. MLP 类：前馈神经网络层，用于对每个patch进行特征变换。\n",
    "\n",
    "4. Block 类：Transformer块，包含自注意力层和前馈神经网络层。\n",
    "\n",
    "5. VisionTransformer 类：完整的Vision Transformer模型，包含多个Transformer块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49259685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7967136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "投影后的形状x.shape: torch.Size([1, 768, 14, 14])\n",
      "转换后的形状x.shape: torch.Size([1, 196, 768])\n",
      "输出形状out.shape: torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    对2D图像作Patch Embedding操作\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        \"\"\"\n",
    "        初始化PatchEmbed层\n",
    "        Args:\n",
    "            img_size (int): 输入图像的大小，默认224\n",
    "            patch_size (int): 每个patch的大小，默认16x16\n",
    "            in_c (int): 输入图像的通道数，默认3\n",
    "            embed_dim (int): 输出的patch embedding维度，默认768\n",
    "            norm_layer (nn.Module, optional): 归一化层，默认None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        img_size=(img_size, img_size)\n",
    "        patch_size=(patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size=(img_size[0]//patch_size[0], img_size[1]//patch_size[1])#计算每个patch的尺寸，图像的长，宽//patch_size的长，宽\n",
    "        self.num_patches=self.grid_size[0]*self.grid_size[1]#计算patch的数量，长*宽\n",
    "        #定义卷积层\n",
    "        self.proj=nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.in_c = in_c\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm_layer = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x: [B, C, H, W]\n",
    "        #out: [B, num_patches, embed_dim]\n",
    "        #首先判断输入图像的尺寸是否符合要求\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x=self.proj(x)#通过卷积层，将图像转换为patch embedding\n",
    "        print(f\"投影后的形状x.shape: {x.shape}\")\n",
    "        #将输出的维度从[B, embed_dim, grid_h, grid_w]转换为[B, num_patches, embed_dim]\n",
    "        x=x.flatten(2).transpose(1, 2)\n",
    "        print(f\"转换后的形状x.shape: {x.shape}\")\n",
    "        #layernorm通常是针对每个patch的embedding进行归一化，而不是针对所有patch的embedding进行归一化\n",
    "        x=self.norm_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#简单测试\n",
    "#创建一个PatchEmbed层\n",
    "patch_embed=PatchEmbed(img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=nn.LayerNorm)\n",
    "#创建一个随机输入张量\n",
    "x=torch.randn(1, 3, 224, 224)\n",
    "#前向传播\n",
    "out=patch_embed(x)\n",
    "print(f\"输出形状out.shape: {out.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec98e5",
   "metadata": {},
   "source": [
    "# 多头自注意力机制 (Multi-Head Self-Attention)\n",
    "\n",
    "## 1. 核心概念\n",
    "**多头自注意力 (MSA)** 是 Transformer 架构的核心组件。它允许模型在处理序列（如图像块 Patch）中的每个元素时，能够同时关注序列中的其他所有位置，从而捕捉**全局上下文信息**。\n",
    "\n",
    "与传统的卷积神经网络 (CNN) 关注局部特征不同，Attention 机制天生具有全局感受野。\n",
    "\n",
    "## 2. 数学原理 (Scaled Dot-Product Attention)\n",
    "\n",
    "标准的自注意力计算公式如下：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "其中：\n",
    "*   **$Q$ (Query)**: 查询向量，代表当前元素正在寻找的信息。\n",
    "*   **$K$ (Key)**: 键向量，代表被查询元素的索引信息。\n",
    "*   **$V$ (Value)**: 值向量，代表被查询元素的实际内容信息。\n",
    "*   **$\\sqrt{d_k}$**: 缩放因子。用于防止 $QK^T$ 的点积结果过大，导致 Softmax 函数进入梯度极小的饱和区（梯度消失）。\n",
    "\n",
    "## 3. 多头机制 (Multi-Head) 的作用\n",
    "\n",
    "**“多头”** 意味着我们将 Query、Key 和 Value 向量分割成 $h$ 个独立的“头”（Head），并在不同的特征子空间中并行计算注意力。\n",
    "\n",
    "*   **为什么要多头？**\n",
    "    这类似于 CNN 中的多个卷积核（Filters）。不同的头可以学习关注不同的特征关系。例如，在图像中，一个头可能关注**形状轮廓**，另一个头可能关注**纹理颜色**，还有一个头可能关注**位置关系**。最后将这些不同的信息拼接起来，模型的表达能力更强。\n",
    "\n",
    "*   **计算步骤**：\n",
    "    1.  **线性投影**：通过全连接层将输入 $X$ 投影得到 $Q, K, V$。\n",
    "    2.  **拆分 (Split)**：将总维度 `dim` 拆分为 `num_heads` $\\times$ `head_dim`。\n",
    "    3.  **并行计算**：对每个头独立计算 Attention Score。\n",
    "    4.  **拼接 (Concat)**：将所有头的输出拼接回原来的维度。\n",
    "    5.  **融合 (Projection)**：通过一个全连接层融合不同头的信息。\n",
    "\n",
    "## 4. 代码实现映射\n",
    "\n",
    "对应于 `VisionTransformer.py` 中的 `Attention` 类关键步骤：\n",
    "\n",
    "| 代码操作 | 数学含义 |\n",
    "| :--- | :--- |\n",
    "| `self.qkv(x)` | $W_q, W_k, W_v$ 线性投影生成 Q, K, V |\n",
    "| `.reshape(..., heads, head_dim)` | 拆分为多头 |\n",
    "| `(q @ k.transpose(-2, -1))` | 计算 $QK^T$ 相似度矩阵 |\n",
    "| `* self.scale` | 除以 $\\sqrt{d_k}$ 进行缩放 |\n",
    "| `.softmax(dim=-1)` | 归一化得到注意力权重 |\n",
    "| `attn @ v` | 加权求和得到该头的输出 |\n",
    "| `self.proj(x)` | $W_o$ 输出线性层，融合多头信息 |\n",
    "\n",
    "## 5. 维度变换图解\n",
    "\n",
    "假设输入形状为 `[Batch, N, Dim]`：\n",
    "\n",
    "1.  **Input**: `[B, N, C]`\n",
    "2.  **QKV Projection**: `[B, N, 3*C]` $\\rightarrow$ `[B, N, 3, Heads, Head_Dim]`\n",
    "3.  **Transpose**: `[3, B, Heads, N, Head_Dim]` (分离出 Q, K, V)\n",
    "4.  **Attention Map**: `Q @ K.T` $\\rightarrow$ `[B, Heads, N, N]` (得到 $N \\times N$ 的关系矩阵)\n",
    "5.  **Weighted Value**: `Attn @ V` $\\rightarrow$ `[B, Heads, N, Head_Dim]`\n",
    "6.  **Output**: Reshape & Concat $\\rightarrow$ `[B, N, C]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c801577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q 的前 3×3 内容:\n",
      "tensor([[[-0.5844,  0.1977, -0.2092],\n",
      "         [-0.2171, -0.1312, -1.0171],\n",
      "         [-0.4126,  0.4842,  0.9619]]], grad_fn=<SelectBackward0>)\n",
      "k 的前 3×3 内容:\n",
      "tensor([[[-1.3133, -0.0220,  0.4310],\n",
      "         [-0.1495, -0.6505, -0.3297],\n",
      "         [-0.6323, -0.4088,  0.7506]]], grad_fn=<SelectBackward0>)\n",
      "v 的前 3×3 内容:\n",
      "tensor([[[ 0.6186, -0.6955,  0.0465],\n",
      "         [-0.4180, -0.7046,  0.1503],\n",
      "         [ 0.0653,  0.3319, -0.6398]]], grad_fn=<SelectBackward0>)\n",
      "q.shape: torch.Size([1, 8, 10, 64])\n",
      "k.transpose(-2, -1).shape: torch.Size([1, 8, 64, 10])\n",
      "attn 的前 3×3 内容:\n",
      "tensor([[[ 0.0107,  0.5636, -0.2711],\n",
      "         [-0.0312, -0.6355, -0.1421],\n",
      "         [ 0.5742,  0.3488, -0.2603]]], grad_fn=<SelectBackward0>)\n",
      "经过归一化后的attn 的前 3×3 内容:\n",
      "tensor([[[0.1215, 0.1237, 0.0735],\n",
      "         [0.0953, 0.0509, 0.0656],\n",
      "         [0.1394, 0.1321, 0.0772]]], grad_fn=<SelectBackward0>)\n",
      "attn@v 的前 3×3 内容:\n",
      "tensor([[[ 0.0151, -0.1505, -0.0181],\n",
      "         [ 0.0109, -0.0081,  0.0405],\n",
      "         [ 0.0816,  0.0406,  0.1166]]], grad_fn=<SelectBackward0>)\n",
      "x的内容: tensor([[[ 0.1113,  0.0151,  0.0337,  ...,  0.0581,  0.1008,  0.2043],\n",
      "         [ 0.1293, -0.1505, -0.1781,  ...,  0.0502,  0.0108,  0.3383],\n",
      "         [ 0.1578, -0.0181, -0.0949,  ...,  0.0870,  0.0409,  0.2458],\n",
      "         ...,\n",
      "         [ 0.1981, -0.0453, -0.0197,  ...,  0.1436,  0.0734,  0.1740],\n",
      "         [ 0.2588, -0.0240, -0.0018,  ...,  0.0324,  0.1251,  0.3103],\n",
      "         [ 0.1856,  0.0172, -0.1398,  ...,  0.0965,  0.0530,  0.2737]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "投影层后的x 的前 3×3 内容:\n",
      "tensor([[0.1127, 0.0175, 0.1381]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "#多头自注意力机制的实现\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, num_heads=8, qkv_bias=False, \n",
    "        qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        '''\n",
    "        此函数用于初始化相关参数\n",
    "        :param dim: 输入token的维度\n",
    "        :param num_heads: 注意力多头数量\n",
    "        :param qkv_bias: 是否使用偏置，默认False\n",
    "        :param qk_scale: 缩放因子\n",
    "        :param attn_drop_ratio: 注意力的比例\n",
    "        :param proj_drop_ratio: 投影的比例\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads #每个头注意的维度\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        #qkv：一个全连接层同时计算Q、K、V\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        #随机在计算后的注意力矩阵上进行dropout，防止过拟合\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        #多头注意力拼接后，来统筹不同头的注意力结果的投影层\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        #投影层的dropout\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        此函数用于前向传播\n",
    "        :param x: 输入的token序列，形状为(batch_size, seq_len, dim)\n",
    "        :return: 输出的token序列，形状为(batch_size, seq_len, dim)\n",
    "        '''\n",
    "        B,N,C=x.shape\n",
    "        #qkv:同时计算三个矩阵Q、K、V\n",
    "        qkv=self.qkv(x) #[B,num_patches+1,3*total_dim]\n",
    "        qkv=qkv.reshape(B,N,3,self.num_heads,C//self.num_heads)#[B,num_patches+1,3,head_dim,num_dim_per_heads]\n",
    "        qkv=qkv.permute(2,0,3,1,4) #[3,B,num_heads,num_patches+1,head_dim]\n",
    "        q,k,v=qkv[0],qkv[1],qkv[2] #[B,num_heads,num_patches+1,head_dim]\n",
    "        # 打印 q 的前 3×3 子矩阵（即前 3 行、前 3 列）\n",
    "        print(f\"q 的前 3×3 内容:\\n{q[..., :3, :3,1]}\")\n",
    "        # 打印 k 的前 3×3 子矩阵（即前 3 行、前 3 列）\n",
    "        print(f\"k 的前 3×3 内容:\\n{k[..., :3, :3,1]}\")\n",
    "        # 打印 v 的前 3×3 子矩阵（即前 3 行、前 3 列）\n",
    "        print(f\"v 的前 3×3 内容:\\n{v[..., :3, :3,1]}\")\n",
    "\n",
    "        #计算注意力矩阵\n",
    "        # q: [B, H, N, D]\n",
    "        # k: [B, H, N, D] -> k.transpose(-2, -1): [B, H, D, N],倒数第一个和倒数第二个维度互换位置\n",
    "        # @ 是矩阵乘法\n",
    "        print(f\"q.shape: {q.shape}\")\n",
    "        print(f\"k.transpose(-2, -1).shape: {k.transpose(-2, -1).shape}\")\n",
    "        attn=(q@k.transpose(-2,-1))*self.scale\n",
    "        print(f\"attn 的前 3×3 内容:\\n{attn[..., :3, :3,1]}\")\n",
    "        #对注意力矩阵进行softmax归一化,沿着最后一个维度归一化,将原始的得分转换成概率分布\n",
    "        attn=attn.softmax(dim=-1)\n",
    "        print(f\"经过归一化后的attn 的前 3×3 内容:\\n{attn[..., :3, :3,1]}\")\n",
    "        #进行drop\n",
    "        attn=self.attn_drop(attn)\n",
    "\n",
    "        #加权求和\n",
    "        # attn: [B, H, N, N]\n",
    "        # v:    [B, H, N, D]\n",
    "        # @ 是矩阵乘法\n",
    "        x=(attn@v).transpose(1,2).reshape(B,N,C) \n",
    "        print(f\"attn@v 的前 3×3 内容:\\n{(attn@v)[..., :3, :3,1]}\")\n",
    "        print(f\"x的内容: {x}\")\n",
    "        #[B,num_patches+1,num_heads,head_dim] -> [B,num_patches+1,total_dim]\n",
    "        #投影层\n",
    "        x=self.proj(x)\n",
    "        print(f\"投影层后的x 的前 3×3 内容:\\n{x[..., :3, :3,1]}\")\n",
    "        #dropout\n",
    "        x=self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "#简单测试\n",
    "x=torch.randn(1,10,512)\n",
    "attn=Attention(512)\n",
    "print(attn(x).shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
